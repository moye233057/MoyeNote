```

```

```

```
## 三、根据爬取行为进行反爬，通常在爬取步骤上做分析
```
2.1 通过js实现跳转来反爬
反爬原理: js实现页面跳转，无法在源码中获取下一页url
解决办法: 多次抓包获取条状url，分析规律

2.2 通过蜜罐（陷阱）获取爬虫ip（或者代理ip），进行反爬
反爬原理: 在爬虫获取链接进行请求的过程中，爬虫会根据正则，xpath，css等方式进行后续链接的提取，此时服务端可以设置一个陷阱url，会被提取规则获取，但是正常用户无法获取，这样就能有效区分爬虫和正常用户

2.3 通过假数据反爬
反爬原理: 想返回的响应中添加假数据污染数据库，通常假数据不会被正常用户看到
解决方法: 长期运行，核对数据库中数据同实质页面中数据对应情况，如果存在问题/仔细分析响应内容

2.4 阻塞任务队列
反爬原理: 通过生成大量垃圾url，从而阻塞任务队列，降低爬虫的实际工作效率
解决方法: 观察运行过程中请求响应状态/仔细分析源码获取垃圾url生成规则，对URL进行过滤

2.5 阻塞网络IO
反爬原理: 发送请求获取相应的过程实际上就是下载过程，在任务队列中混入一个大文件url，当爬虫在进行该请求时将会占用网络io，如果是多线程则会占用线程
解决方法: 观察爬虫运行状态/多线程对请求线程时/发送请求线程时的状态

2
```